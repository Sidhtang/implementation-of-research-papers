{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH9Ql4tK5cH67CLQtY2QzD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/implementation-of-research-papers/blob/main/reinforcement_learning_with_human_feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG5nCOOIobuu"
      },
      "outputs": [],
      "source": [
        "# @title Proximal policy optimization and policygradient\n",
        "\n",
        "# It works like a neural network architecture, whereby the gradient of the output, i.e,\n",
        "#the log of probabilities of actions in that particular state, is taken with respect to parameters of the environment\n",
        "# and the change is reflected in the policy, based upon the gradients.\n",
        "# the problem with this mwthod wast that hypersenstivity and hperparameter\n",
        "# along with their poor sample efficiency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "class PPOMemory:\n",
        "    def __init__(self, batch_size):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def store_memory(self, state, action, probs, vals, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(probs)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "        return np.array(self.states), np.array(self.actions), \\\n",
        "               np.array(self.probs), np.array(self.vals), \\\n",
        "               np.array(self.rewards), np.array(self.dones), batches\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, n_actions, alpha, fc1_dims=256, fc2_dims=256):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims, fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc2_dims, n_actions),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        dist = self.actor(state)\n",
        "        return dist\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(input_dims, fc1_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc1_dims, fc2_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(fc2_dims, 1)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "        return value\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, input_dims, n_actions, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
        "                 policy_clip=0.2, batch_size=64, n_epochs=10):\n",
        "        self.gamma = gamma\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "\n",
        "        self.actor = ActorNetwork(input_dims, n_actions, alpha)\n",
        "        self.critic = CriticNetwork(input_dims, alpha)\n",
        "        self.memory = PPOMemory(batch_size)\n",
        "\n",
        "    def remember(self, state, action, probs, vals, reward, done):\n",
        "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        torch.save(self.actor.state_dict(), 'actor.pth')\n",
        "        torch.save(self.critic.state_dict(), 'critic.pth')\n",
        "\n",
        "    def load_models(self):\n",
        "        self.actor.load_state_dict(torch.load('actor.pth'))\n",
        "        self.critic.load_state_dict(torch.load('critic.pth'))\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = torch.tensor([observation], dtype=torch.float).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        action = dist.sample()\n",
        "\n",
        "        probs = torch.squeeze(dist).detach().cpu().numpy()\n",
        "        action = torch.squeeze(action).detach().cpu().numpy()\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return action, probs, value\n",
        "\n",
        "    def learn(self):\n",
        "        for _ in range(self.n_epochs):\n",
        "            state_arr, action_arr, old_prob_arr, vals_arr, \\\n",
        "            reward_arr, dones_arr, batches = self.memory.generate_batches()\n",
        "\n",
        "            values = vals_arr\n",
        "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
        "\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                discount = 1\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
        "                            (1-int(dones_arr[k])) - values[k])\n",
        "                    discount *= self.gamma*self.gae_lambda\n",
        "                advantage[t] = a_t\n",
        "\n",
        "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
        "            values = torch.tensor(values).to(self.actor.device)\n",
        "\n",
        "            for batch in batches:\n",
        "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(self.actor.device)\n",
        "                old_probs = torch.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
        "                actions = torch.tensor(action_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "                critic_value = torch.squeeze(critic_value)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "                prob_ratio = new_probs.exp() / old_probs\n",
        "                weighted_probs = advantage[batch] * prob_ratio\n",
        "                weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip,\n",
        "                        1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = (returns-critic_value)**2\n",
        "                critic_loss = critic_loss.mean()\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "        self.memory.clear_memory()"
      ],
      "metadata": {
        "id": "WwgGA2U8pZCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb65KUB3pZE0",
        "outputId": "d7f0171f-5add-4766-b134-2784d14f0ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/958.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# [Previous PPOMemory, ActorNetwork, and CriticNetwork classes remain the same]\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0001, gae_lambda=0.95,\n",
        "                 policy_clip=0.2, batch_size=64, n_epochs=10):\n",
        "        self.gamma = gamma\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "\n",
        "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
        "        self.critic = CriticNetwork(input_dims, alpha)\n",
        "        self.memory = PPOMemory(batch_size)\n",
        "\n",
        "    def remember(self, state, action, probs, vals, reward, done):\n",
        "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = torch.FloatTensor(observation).unsqueeze(0).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        action = dist.sample()\n",
        "\n",
        "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
        "        action = torch.squeeze(action).item()\n",
        "        value = torch.squeeze(value).item()\n",
        "\n",
        "        return action, probs, value\n",
        "\n",
        "    def learn(self):\n",
        "        total_actor_loss = 0\n",
        "        total_critic_loss = 0\n",
        "\n",
        "        for _ in range(self.n_epochs):\n",
        "            state_arr, action_arr, old_prob_arr, vals_arr, \\\n",
        "            reward_arr, dones_arr, batches = \\\n",
        "                    self.memory.generate_batches()\n",
        "\n",
        "            values = vals_arr\n",
        "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
        "\n",
        "            # Calculate advantages using GAE\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                discount = 1\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
        "                            (1-int(dones_arr[k])) - values[k])\n",
        "                    discount *= self.gamma*self.gae_lambda\n",
        "                advantage[t] = a_t\n",
        "\n",
        "            # Normalize advantages\n",
        "            advantage = torch.FloatTensor(advantage).to(self.actor.device)\n",
        "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
        "\n",
        "            values = torch.FloatTensor(values).to(self.actor.device)\n",
        "\n",
        "            for batch in batches:\n",
        "                states = torch.FloatTensor(state_arr[batch]).to(self.actor.device)\n",
        "                old_probs = torch.FloatTensor(old_prob_arr[batch]).to(self.actor.device)\n",
        "                actions = torch.FloatTensor(action_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "                critic_value = torch.squeeze(critic_value)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "                prob_ratio = (new_probs - old_probs).exp()\n",
        "                weighted_probs = advantage[batch] * prob_ratio\n",
        "                weighted_clipped_probs = advantage[batch] * \\\n",
        "                        torch.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip)\n",
        "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = (returns-critic_value)**2\n",
        "                critic_loss = critic_loss.mean()\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss\n",
        "\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=0.5)\n",
        "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=0.5)\n",
        "\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "                total_actor_loss += actor_loss.item()\n",
        "                total_critic_loss += critic_loss.item()\n",
        "\n",
        "        self.memory.clear_memory()\n",
        "        return total_actor_loss / self.n_epochs, total_critic_loss / self.n_epochs\n",
        "\n",
        "def train():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    N = 1000  # Increased maximum episodes\n",
        "    batch_size = 64  # Increased batch size\n",
        "    n_epochs = 5\n",
        "    alpha = 0.0001  # Learning rate\n",
        "\n",
        "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size,\n",
        "                 alpha=alpha, n_epochs=n_epochs,\n",
        "                 input_dims=env.observation_space.shape[0],\n",
        "                 gamma=0.99,\n",
        "                 gae_lambda=0.95,\n",
        "                 policy_clip=0.2)\n",
        "\n",
        "    if not os.path.exists('plots'):\n",
        "        os.makedirs('plots')\n",
        "\n",
        "    figure_file = 'plots/cartpole.png'\n",
        "    best_score = float('-inf')\n",
        "    score_history = []\n",
        "    loss_history = {'actor': [], 'critic': []}\n",
        "\n",
        "    learn_iters = 0\n",
        "    avg_score = 0\n",
        "    n_steps = 0\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 150  # Increased patience\n",
        "    best_avg_score = float('-inf')\n",
        "    episodes_without_improvement = 0\n",
        "    min_episodes = 200  # Minimum episodes before early stopping\n",
        "\n",
        "    for i in range(N):\n",
        "        observation, _ = env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action, prob, val = agent.choose_action(observation)\n",
        "            new_observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            # Modified reward to encourage longer episodes\n",
        "            modified_reward = reward\n",
        "            if terminated and episode_steps < 500:  # Max steps for CartPole-v1\n",
        "                modified_reward = -1\n",
        "\n",
        "            done = terminated or truncated\n",
        "            n_steps += 1\n",
        "            episode_steps += 1\n",
        "            score += reward  # Keep original reward for score\n",
        "            agent.remember(observation, action, prob, val, modified_reward, done)\n",
        "\n",
        "            if n_steps % batch_size == 0:\n",
        "                actor_loss, critic_loss = agent.learn()\n",
        "                loss_history['actor'].append(actor_loss)\n",
        "                loss_history['critic'].append(critic_loss)\n",
        "                learn_iters += 1\n",
        "\n",
        "            observation = new_observation\n",
        "\n",
        "        score_history.append(score)\n",
        "        avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "\n",
        "        # Early stopping check with minimum episodes requirement\n",
        "        if i >= min_episodes and avg_score > best_avg_score:\n",
        "            best_avg_score = avg_score\n",
        "            episodes_without_improvement = 0\n",
        "        elif i >= min_episodes:\n",
        "            episodes_without_improvement += 1\n",
        "\n",
        "        if i >= min_episodes and episodes_without_improvement >= patience:\n",
        "            print(f'Early stopping triggered at episode {i} with best average score: {best_avg_score:.1f}')\n",
        "            break\n",
        "\n",
        "        if i % 10 == 0:  # Print every 10 episodes\n",
        "            print(f'Episode: {i} Score: {score:.1f} Avg Score: {avg_score:.1f} Best: {best_avg_score:.1f}')\n",
        "\n",
        "    # Plot learning curves\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(score_history)\n",
        "    plt.title('Learning Curve')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(loss_history['actor'], label='Actor Loss')\n",
        "    plt.plot(loss_history['critic'], label='Critic Loss')\n",
        "    plt.title('Loss History')\n",
        "    plt.xlabel('Update Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figure_file)\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6tlL5CpxrHI",
        "outputId": "29fcadff-9aa9-4672-81a8-1b52948e1caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Score: 10.0 Avg Score: 10.0 Best: -inf\n",
            "Episode: 10 Score: 12.0 Avg Score: 17.6 Best: -inf\n",
            "Episode: 20 Score: 14.0 Avg Score: 24.7 Best: -inf\n",
            "Episode: 30 Score: 35.0 Avg Score: 25.9 Best: -inf\n",
            "Episode: 40 Score: 29.0 Avg Score: 26.2 Best: -inf\n",
            "Episode: 50 Score: 29.0 Avg Score: 26.1 Best: -inf\n",
            "Episode: 60 Score: 48.0 Avg Score: 25.9 Best: -inf\n",
            "Episode: 70 Score: 15.0 Avg Score: 25.7 Best: -inf\n",
            "Episode: 80 Score: 14.0 Avg Score: 25.2 Best: -inf\n",
            "Episode: 90 Score: 25.0 Avg Score: 25.2 Best: -inf\n",
            "Episode: 100 Score: 22.0 Avg Score: 25.1 Best: -inf\n",
            "Episode: 110 Score: 30.0 Avg Score: 26.3 Best: -inf\n",
            "Episode: 120 Score: 18.0 Avg Score: 25.4 Best: -inf\n",
            "Episode: 130 Score: 15.0 Avg Score: 24.2 Best: -inf\n",
            "Episode: 140 Score: 29.0 Avg Score: 24.0 Best: -inf\n",
            "Episode: 150 Score: 43.0 Avg Score: 24.3 Best: -inf\n",
            "Episode: 160 Score: 23.0 Avg Score: 25.1 Best: -inf\n",
            "Episode: 170 Score: 33.0 Avg Score: 26.9 Best: -inf\n",
            "Episode: 180 Score: 68.0 Avg Score: 28.6 Best: -inf\n",
            "Episode: 190 Score: 48.0 Avg Score: 29.5 Best: -inf\n",
            "Episode: 200 Score: 41.0 Avg Score: 31.4 Best: 31.4\n",
            "Episode: 210 Score: 66.0 Avg Score: 34.2 Best: 34.2\n",
            "Episode: 220 Score: 22.0 Avg Score: 35.6 Best: 35.6\n",
            "Episode: 230 Score: 37.0 Avg Score: 38.5 Best: 38.5\n",
            "Episode: 240 Score: 24.0 Avg Score: 41.5 Best: 41.5\n",
            "Episode: 250 Score: 43.0 Avg Score: 42.5 Best: 42.6\n",
            "Episode: 260 Score: 33.0 Avg Score: 42.2 Best: 42.9\n",
            "Episode: 270 Score: 43.0 Avg Score: 41.2 Best: 42.9\n",
            "Episode: 280 Score: 20.0 Avg Score: 40.8 Best: 42.9\n",
            "Episode: 290 Score: 34.0 Avg Score: 41.1 Best: 42.9\n",
            "Episode: 300 Score: 28.0 Avg Score: 42.3 Best: 42.9\n",
            "Episode: 310 Score: 50.0 Avg Score: 40.2 Best: 42.9\n",
            "Episode: 320 Score: 53.0 Avg Score: 41.9 Best: 42.9\n",
            "Episode: 330 Score: 14.0 Avg Score: 42.6 Best: 43.2\n",
            "Episode: 340 Score: 39.0 Avg Score: 42.6 Best: 43.6\n",
            "Episode: 350 Score: 65.0 Avg Score: 45.0 Best: 45.0\n",
            "Episode: 360 Score: 23.0 Avg Score: 48.4 Best: 48.5\n",
            "Episode: 370 Score: 16.0 Avg Score: 47.7 Best: 48.5\n",
            "Episode: 380 Score: 28.0 Avg Score: 47.1 Best: 48.5\n",
            "Episode: 390 Score: 45.0 Avg Score: 47.0 Best: 48.5\n",
            "Episode: 400 Score: 43.0 Avg Score: 45.4 Best: 48.5\n",
            "Episode: 410 Score: 14.0 Avg Score: 44.5 Best: 48.5\n",
            "Episode: 420 Score: 20.0 Avg Score: 42.6 Best: 48.5\n",
            "Episode: 430 Score: 13.0 Avg Score: 40.6 Best: 48.5\n",
            "Episode: 440 Score: 24.0 Avg Score: 38.7 Best: 48.5\n",
            "Episode: 450 Score: 27.0 Avg Score: 34.8 Best: 48.5\n",
            "Episode: 460 Score: 23.0 Avg Score: 31.5 Best: 48.5\n",
            "Episode: 470 Score: 23.0 Avg Score: 31.9 Best: 48.5\n",
            "Episode: 480 Score: 18.0 Avg Score: 31.9 Best: 48.5\n",
            "Episode: 490 Score: 21.0 Avg Score: 31.3 Best: 48.5\n",
            "Episode: 500 Score: 20.0 Avg Score: 30.1 Best: 48.5\n",
            "Early stopping triggered at episode 508 with best average score: 48.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.distributions import Categorical\n",
        "from typing import List, Tuple\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x)\n",
        "\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.network(x)\n",
        "\n",
        "class RLHF:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        learning_rate: float = 3e-4,\n",
        "        gamma: float = 0.99,\n",
        "        epsilon: float = 0.2,\n",
        "        c1: float = 1.0,\n",
        "        c2: float = 0.01\n",
        "    ):\n",
        "        self.policy = PolicyNetwork(input_dim, hidden_dim, output_dim)\n",
        "        self.value = ValueNetwork(input_dim, hidden_dim)\n",
        "        self.reward_model = RewardModel(input_dim, hidden_dim)\n",
        "\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
        "        self.value_optimizer = optim.Adam(self.value.parameters(), lr=learning_rate)\n",
        "        self.reward_model_optimizer = optim.Adam(self.reward_model.parameters(), lr=learning_rate)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon  # PPO clipping parameter\n",
        "        self.c1 = c1  # Value loss coefficient\n",
        "        self.c2 = c2  # Entropy coefficient\n",
        "\n",
        "    def train_reward_model(self, demonstrations: List[Tuple[torch.Tensor, float]]):\n",
        "        \"\"\"Train reward model on human feedback demonstrations\"\"\"\n",
        "        self.reward_model.train()\n",
        "\n",
        "        for state, human_reward in demonstrations:\n",
        "            predicted_reward = self.reward_model(state)\n",
        "            loss = nn.MSELoss()(predicted_reward, torch.tensor([human_reward]))\n",
        "\n",
        "            self.reward_model_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.reward_model_optimizer.step()\n",
        "\n",
        "    def compute_gae(\n",
        "        self,\n",
        "        rewards: List[float],\n",
        "        values: List[float],\n",
        "        next_value: float,\n",
        "        dones: List[bool],\n",
        "        gamma: float = 0.99,\n",
        "        lam: float = 0.95\n",
        "    ) -> List[float]:\n",
        "        \"\"\"Compute Generalized Advantage Estimation\"\"\"\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "\n",
        "        for r, v, done, next_v in zip(\n",
        "            reversed(rewards),\n",
        "            reversed(values),\n",
        "            reversed(dones),\n",
        "            reversed(values[1:] + [next_value])\n",
        "        ):\n",
        "            delta = r + gamma * next_v * (1 - done) - v\n",
        "            gae = delta + gamma * lam * (1 - done) * gae\n",
        "            advantages.insert(0, gae)\n",
        "\n",
        "        return advantages\n",
        "\n",
        "    def ppo_update(\n",
        "        self,\n",
        "        states: torch.Tensor,\n",
        "        actions: torch.Tensor,\n",
        "        old_log_probs: torch.Tensor,\n",
        "        advantages: torch.Tensor,\n",
        "        returns: torch.Tensor,\n",
        "        epochs: int = 10\n",
        "    ):\n",
        "        \"\"\"Update policy using PPO algorithm\"\"\"\n",
        "        for _ in range(epochs):\n",
        "            # Get current policy distributions\n",
        "            action_probs = self.policy(states)\n",
        "            dist = Categorical(action_probs)\n",
        "            curr_log_probs = dist.log_prob(actions)\n",
        "\n",
        "            # Calculate ratio and surrogate losses\n",
        "            ratios = torch.exp(curr_log_probs - old_log_probs)\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
        "\n",
        "            # Calculate policy loss with clipping\n",
        "            policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "            # Calculate value loss\n",
        "            value_pred = self.value(states)\n",
        "            value_loss = nn.MSELoss()(value_pred, returns)\n",
        "\n",
        "            # Calculate entropy bonus\n",
        "            entropy = dist.entropy().mean()\n",
        "\n",
        "            # Combined loss\n",
        "            total_loss = (\n",
        "                policy_loss +\n",
        "                self.c1 * value_loss -\n",
        "                self.c2 * entropy\n",
        "            )\n",
        "\n",
        "            # Update networks\n",
        "            self.policy_optimizer.zero_grad()\n",
        "            self.value_optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            self.policy_optimizer.step()\n",
        "            self.value_optimizer.step()\n",
        "\n",
        "    def collect_trajectory(self, env, max_steps: int = 1000) -> Tuple[List]:\n",
        "        \"\"\"Collect a trajectory using current policy\"\"\"\n",
        "        states, actions, rewards, values, log_probs, dones = [], [], [], [], [], []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps:\n",
        "            # Convert state to tensor and get action from policy\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            action_probs = self.policy(state_tensor)\n",
        "            dist = Categorical(action_probs)\n",
        "            action = dist.sample()\n",
        "\n",
        "            # Get value estimate and log probability\n",
        "            value = self.value(state_tensor)\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "            # Take action in environment\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "            # Get reward from reward model\n",
        "            predicted_reward = self.reward_model(state_tensor).item()\n",
        "\n",
        "            # Store transition\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(predicted_reward)  # Use reward model's prediction\n",
        "            values.append(value.item())\n",
        "            log_probs.append(log_prob)\n",
        "            dones.append(done)\n",
        "\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "        return states, actions, rewards, values, log_probs, dones\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        env,\n",
        "        n_episodes: int = 1000,\n",
        "        max_steps: int = 1000,\n",
        "        update_freq: int = 10\n",
        "    ):\n",
        "        \"\"\"Main training loop\"\"\"\n",
        "        for episode in range(n_episodes):\n",
        "            # Collect trajectory\n",
        "            states, actions, rewards, values, log_probs, dones = self.collect_trajectory(env, max_steps)\n",
        "\n",
        "            # Convert to tensors\n",
        "            states = torch.FloatTensor(states)\n",
        "            actions = torch.LongTensor(actions)\n",
        "            old_log_probs = torch.stack(log_probs)\n",
        "\n",
        "            # Compute returns and advantages\n",
        "            next_value = self.value(torch.FloatTensor(states[-1])).item()\n",
        "            advantages = self.compute_gae(rewards, values, next_value, dones)\n",
        "            returns = torch.FloatTensor(advantages) + torch.FloatTensor(values)\n",
        "            advantages = torch.FloatTensor(advantages)\n",
        "\n",
        "            # Normalize advantages\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "            # PPO update\n",
        "            if episode % update_freq == 0:\n",
        "                self.ppo_update(states, actions, old_log_probs, advantages, returns)\n",
        "\n",
        "# Example usage:\n",
        "\"\"\"\n",
        "# Initialize environment and RLHF\n",
        "env = gym.make('CartPole-v1')\n",
        "rlhf = RLHF(\n",
        "    input_dim=4,  # CartPole state dimension\n",
        "    hidden_dim=64,\n",
        "    output_dim=2  # CartPole action dimension\n",
        ")\n",
        "\n",
        "# Create synthetic human feedback for demonstration\n",
        "demonstrations = [\n",
        "    (torch.randn(4), 1.0),  # (state, human_reward)\n",
        "    (torch.randn(4), -1.0),\n",
        "    # ... more demonstrations\n",
        "]\n",
        "\n",
        "# Train reward model\n",
        "rlhf.train_reward_model(demonstrations)\n",
        "\n",
        "# Train policy using RLHF\n",
        "rlhf.train(env)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "6i77S7qi1KT6",
        "outputId": "535dc1c6-efe4-4d80-80cc-a5b6c31bfc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Initialize environment and RLHF\\nenv = gym.make('CartPole-v1')\\nrlhf = RLHF(\\n    input_dim=4,  # CartPole state dimension\\n    hidden_dim=64,\\n    output_dim=2  # CartPole action dimension\\n)\\n\\n# Create synthetic human feedback for demonstration\\ndemonstrations = [\\n    (torch.randn(4), 1.0),  # (state, human_reward)\\n    (torch.randn(4), -1.0),\\n    # ... more demonstrations\\n]\\n\\n# Train reward model\\nrlhf.train_reward_model(demonstrations)\\n\\n# Train policy using RLHF\\nrlhf.train(env)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "57t3j-a91KWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWKXS9DX1KZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m0zeKecB1Kcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZaD_hPAG1Kf7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}